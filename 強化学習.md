---
tags:
  - 強化学習
  - study
---

# 2020-10-31 強化学習

OpenAI Gym / Baselines 深層学習・強化学習 人工知能プログラミング 実践入門
`https://www.borndigital.co.jp/book/17130.html`(正誤表、サンプルコードが入っている)

【sample】
work_folder : %work_path%/open_ai_gym/sample/sample

【jupyter 起動】

```jupyter
conda activate openai_gym
cd 000_work/Study_AI
jupyter notebook
```

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [2020-10-31 強化学習](#2020-10-31-強化学習)
  - [環境作り](#環境作り)
  - [学習状況を確認する](#学習状況を確認する)
    - [ログファイルを出力する方法](#ログファイルを出力する方法)
    - [アウトプットの説明](#アウトプットの説明)
    - [ログファイルの説明](#ログファイルの説明)
    - [tensorboard の起動](#tensorboard-の起動)
  - [モデルの保存と読み込み](#モデルの保存と読み込み)
  - [カスタム gym の環境作成](#カスタム-gym-の環境作成)
    - [概要](#概要)
    - [gym の調査](#gym-の調査)
      - [クラスの定義](#クラスの定義)
      - [動かすのに必要なメソッドとプロパティ](#動かすのに必要なメソッドとプロパティ)
      - [行動空間(action_space)と状態空間(observation_space)の型の定義](#行動空間action_spaceと状態空間observation_spaceの型の定義)
  - [方策](#方策)
  - [後で読む](#後で読む)

<!-- /code_chunk_output -->

## 環境作り

- 仮想環境を作成
  本に習って、python3.6 の環境を作成する

  ```create_env
  conda create --name openai_gym python=3.6 anaconda
  ```

- openAI gym のインストール

  - windows
    atari は windows では非公式、入れるためには、microsoft build tools 2015 が必要
    `https://www.microsoft.com/ja-JP/download/details.aspx?id=48159`から入れる

    ```pip
    conda activate openai_gym
    pip install gym
    pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
    ```

  - linux

    atari が win10 で動かなかったので,WSL で Linux で学習を進める事にした。
    [WSL 環境整備](./2020-10-31 linux.md)

    linux の場合は、下記のコマンドを実行して、必要なソフトを入れる

    ```bash
    sudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev
    ```

    ```pip
    conda activate openai_gym
    pip install gym
    pip install gym[atari]
    pip install gym[box2D]
    ```

- ランダムに動かしてテスト ---> 無事に動いた

  ```python
  import gym
  env = gym.make("CartPole-v1")
  env.reset()
  while True:
      env.render()
      action = env.action_space.sample()
      state,reward,done,info = env.step(action)
      if done:
          break
  env.close()
  ```

- Stable Baselines のインストール

  - windows
    windows にインストールする場合は、Microsoft MPI v10.0 が必要になるので、
    `https://www.microsoft.com/en-us/download/details.aspx?id=57467`から`msmpisetup.exe`をダウンロードしてインストール  
     ※exe でないと、上手くいかなかった。

  ```pip
  conda activate openai_gym
  pip install stable-baselines[mpi]
  pip install tensorflow==1.14.0
  pip install pyqt5
  pip install imageio
  ```

  test ---> 無事に動いた

  ```python
  import gym
  from stable_baselines.common.vec_env import DummyVecEnv
  from stable_baselines import PPO2

  env = gym.make('CartPole-v1')
  env = DummyVecEnv([lambda: env])
  model = PPO2("MlpPolicy", env, verbose=1)
  model.learn(total_timesteps=10000)
  state = env.reset()

  for i in range(1000):
      env.render()
      action, _ = model.predict(state,deterministic=True)
      state, rewards, done, info = env.step(action)
      if done:
          break
  env.close()
  ```

## 学習状況を確認する

学習がどのように進んでいるかを確認する

### ログファイルを出力する方法

下記を追加の上、学習すると logs の下にログファイル（monitor.csv）が吐き出される

```python
from stable_baselines.bench import Monitor

log_dir = './logs/'
os.makedirs(log_dir, exist_ok=True)
env = Monitor(env, log_dir, allow_early_resets=True) # (2)allow_early_resets：学習前の環境リセット許可の有無

model = PPO2('MlpPolicy', env, verbose=1, tensorboard_log=log_dir) # :verbose=1で訓練情報出力,tensorboard_logでフォルダを指定すると、tensorboardのログがでる
```

※verbose の説明

- 0:なし
- 1:訓練情報
- 2:TensorFlow デバッグ

### アウトプットの説明

```out
--------------------------------------
| approxkl           | 0.00018998244 |　新しい方策から古い方策へのkullback-Leibler発散尺度　←　確率分布がどれだけ似ているか、ということ？
| clipfrac           | 0.0           |　クリップ範囲ハイパーパラメータが使用される回数の割合
| ep_len_mean        | 33.2          |　平均エピソード長：過去のエピソードのエピソード長の平均(ゲームオーバーがあるenvの場合は、長い方がよい)
| ep_reward_mean     | 33.2          |　平均報酬：過去のエピソードの累計報酬の平均（単調増加がGOOD)
| explained_variance | 0.0314        |　誤差の分散
| fps                | 749           |　1秒あたりのフレーム数
| n_updates          | 23            |　更新回数
| policy_entropy     | 0.608039      |　方策のエントロピー
| policy_loss        | 0.0006575803  |
| serial_timesteps   | 2944          |　1つの環境でのタイムステップ数
| time_elapsed       | 4.46          |　経過時間
| total_timesteps    | 2944          |　全環境でのタイムステップ数
| value_loss         | 56.175343     |　価値関数更新時の平均損失
--------------------------------------
```

### ログファイルの説明

ログファイルの例を下記に示す。行は、エピソード、列は["報酬","エピソード長","経過時間"]である

```log.csv
#{"t_start": 1604297918.5566726, "env_id": "CartPole-v1"}
r,l,t

15.0,15,2.209498

10.0,10,2.22249
```

### tensorboard の起動

コマンドプロンプトで下記を実行して、`http://localhost:6006/`にアクセス

```tensorboard
tensorboard --logdir=./logs/
```

## モデルの保存と読み込み

zip ファイルで保存される。保存される内容は下記

- data{JSOM}: ハイパーパラメータの辞書
- paramater_list{JSOM}: params に含まれる重みパラメータのリスト
- paramaters{ZIP}: 重みパラメータのバイナリファイル群

モデルの保存

```save.py
model.save('sample')
```

モデルの読み込み

```load.py
from stable_baselines import PPO2
model = PPO2.load('sample')
```

## カスタム gym の環境作成

### 概要

| 目的           | 左に移動                               |
| -------------- | -------------------------------------- |
| 行動           | 0：左に移動、1：右に移動               |
| 状態           | 現在の位置：（左）0 ～ 4（右）         |
| 報酬           | ゴール時：+1 　、　 1 ステップ毎に-0.1 |
| エピソード完了 | ゴール時                               |
| 初期状態       | 右端                                   |

### gym の調査

#### クラスの定義

gym.Env のクラスを継承してクラスを作成する
`class GoLeft(gym.Env):`

#### 動かすのに必要なメソッドとプロパティ

gym.Env のクラスは以下のメソッドとプロパティを実装する必要があり

| メソッド                                | 解説                               |
| --------------------------------------- | ---------------------------------- |
| setp(self, action)                      | action を実行し、結果を返す        |
| reset(self)                             | 状態を初期化し、初期の観測値を返す |
| render(self, mode='human', close=False) | 環境を可視化する                   |
| close(self)                             | 環境を閉じて後処理をする           |
| seed(self, seed=none)                   | ランダムシードを固定する           |

| プロパティ        | 解説                          |
| ----------------- | ----------------------------- |
| action_space      | 行動(Action)の張る空間        |
| observation_space | 観測値(Observation)の張る空間 |
| reward_range      | 報酬の最小値を最大値のリスト  |

#### 行動空間(action_space)と状態空間(observation_space)の型の定義

「OpenAI Gym」は、次の 6 つの空間の型をサポートしています。
「Box」(連続値)と「Discrete」(離散値)が、最も一般的に使用される型になります。特に「状態空間」は多くが「Box」です。「行動空間」は「Discrete」の方が「Box」より、学習が容易になります。

- gym.spaces.Box :主に状態空間として使用される
  範囲[low、high]の連続値、Float 型の n 次元配列。  
  `gym.spaces.Box(low=-100, high=100, shape=(2,))`

- gym.spaces.Discrete

  範囲[0、n-1]の離散値、Int 型の数値。
  `gym.spaces.Discrete(4)`

- ランダムにアクションを取得する

  `env.action_space.sample()`

## 方策

方策とは、状態 s で行動 a を取る確率を表している

| 方策            | 説明                                        |
| --------------- | ------------------------------------------- |
| MlpPolicy       | MLP:多層パーセプトロンを使用する方策        |
| MlpLstmPolicy   | MLP と LSTM を使用する方策                  |
| MlpLnLstmPolicy | MLP と layer normalized LSTM を使用する方策 |
| CnnPolicy       | CNN を使用する方策                          |
| CnnLstmPolicy   | Cnn と LSTM を使用する方策                  |
| CnnLnLstmPolicy | Cnn と layer normalized LSTM を使用する方策 |

## 後で読む

- [強化学習のヒントとコツ](https://note.com/npaka/n/na7a409cbadc7?magazine_key=m3749c0d921b8)
