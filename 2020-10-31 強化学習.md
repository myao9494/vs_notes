---
tags:
  - 強化学習
---

# 2020-10-31 強化学習

OpenAI Gym / Baselines 深層学習・強化学習 人工知能プログラミング 実践入門
`https://www.borndigital.co.jp/book/17130.html`(正誤表、サンプルコードが入っている)

【sample】
work_folder : %work_path%/open_ai_gym/sample/sample

【jupyter 起動】

```jupyter
activate openai_gym
cd 000_work/open_ai_gym
jupyter notebook
```

##　環境作り

- 仮想環境を作成
  本に習って、python3.6 の環境を作成する

  ```create_env
  conda create --name openai_gym python=3.6 anaconda
  ```

- openAI gym のインストール
  atari は windows では非公式、入れるためには、microsoft build tools 2015 が必要
  `https://www.microsoft.com/ja-JP/download/details.aspx?id=48159`から入れる

  ```pip
  activate openai_gym
  pip install gym
  pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py
  ```

  ランダムに動かしてテスト ---> 無事に動いた

  ```test.py
  import gym
  env = gym.make("CartPole-v1")
  env.reset()
  while True:
  env.render()
  action = env.action_space.sample()
  state,reward,done,info = env.step(action)
  if done:
      break
  env.close()
  ```

- Stable Baselines のインストール
  windows にインストールする場合は、Microsoft MPI v10.0 が必要になるので、
  `https://www.microsoft.com/en-us/download/details.aspx?id=57467`から`msmpisetup.exe`をダウンロードしてインストール  
   ※exe でないと、上手くいかなかった。

  ```pip
  activate openai_gym
  pip install stable-baselines[mpi]
  pip install tensorflow==1.14.0
  pip install pyqt5
  pip install imageio
  ```

  test ---> 無事に動いた

  ```test.py
  import gym
  from stable_baselines.common.vec_env import DummyVecEnv
  from stable_baselines import PPO2

  env = gym.make('CartPole-v1')
  env = DummyVecEnv([lambda: env])
  model = PPO2("MlpPolicy", env, verbose=1)
  model.learn(total_timesteps=10000)
  state = env.reset()

  for i in range(1000):
      env.render()
      action, _ = model.predict(state,deterministic=True)
      state, rewards, done, info = env.step(action)
      if done:
          break
  env.close()
  ```

## 学習状況を確認する

学習がどのように進んでいるかを確認する

### ログファイルを出力する方法

下記を追加の上、学習すると logs の下にログファイル（monitor.csv）が吐き出される

```log.py
from stable_baselines.bench import Monitor

log_dir = './logs/'
os.makedirs(log_dir, exist_ok=True)
env = Monitor(env, log_dir, allow_early_resets=True) # (2)allow_early_resets：学習前の環境リセット許可の有無

model = PPO2('MlpPolicy', env, verbose=1, tensorboard_log=log_dir) # :verbose=1で訓練情報出力,tensorboard_logでフォルダを指定すると、tensorboardのログがでる
```

※verboseの説明

- 0:なし
- 1:訓練情報
- 2:TensorFlow デバッグ

### アウトプットの説明

```out
--------------------------------------
| approxkl           | 0.00018998244 |　新しい方策から古い方策へのkullback-Leibler発散尺度　←　確率分布がどれだけ似ているか、ということ？
| clipfrac           | 0.0           |　クリップ範囲ハイパーパラメータが使用される回数の割合
| ep_len_mean        | 33.2          |　平均エピソード長：過去のエピソードのエピソード長の平均(ゲームオーバーがあるenvの場合は、長い方がよい)
| ep_reward_mean     | 33.2          |　平均報酬：過去のエピソードの累計報酬の平均（単調増加がGOOD)
| explained_variance | 0.0314        |　誤差の分散
| fps                | 749           |　1秒あたりのフレーム数
| n_updates          | 23            |　更新回数
| policy_entropy     | 0.608039      |　方策のエントロピー
| policy_loss        | 0.0006575803  |
| serial_timesteps   | 2944          |　1つの環境でのタイムステップ数
| time_elapsed       | 4.46          |　経過時間
| total_timesteps    | 2944          |　全環境でのタイムステップ数
| value_loss         | 56.175343     |　価値関数更新時の平均損失
--------------------------------------
```

### ログファイルの説明

ログファイルの例を下記に示す。行は、エピソード、列は["報酬","エピソード長","経過時間"]である

```log.csv
#{"t_start": 1604297918.5566726, "env_id": "CartPole-v1"}
r,l,t

15.0,15,2.209498

10.0,10,2.22249
```

### tensorboardの起動

コマンドプロンプトで下記を実行して、`http://localhost:6006/`にアクセス

```tensorboard
tensorboard --logdir=./logs/
```



## モデルの保存と読み込み

zip ファイルで保存される。保存される内容は下記

- data{JSOM}: ハイパーパラメータの辞書
- paramater_list{JSOM}: params に含まれる重みパラメータのリスト
- paramaters{ZIP}: 重みパラメータのバイナリファイル群

モデルの保存

```save.py
model.save('sample')
```

モデルの読み込み

```load.py
from stable_baselines import PPO2
model = PPO2.load('sample')
```
